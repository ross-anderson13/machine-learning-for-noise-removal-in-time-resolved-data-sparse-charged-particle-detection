{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "denoising_neural_network.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi\n",
        "\n",
        "#mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "#install keras and tensorflow \n",
        "!pip install keras==2.4.3\n",
        "!pip install tensorflow==2.3.0\n",
        "\n",
        "  \n",
        "#import python packages\n",
        "import os\n",
        "import os.path\n",
        "import tensorflow as tf\n",
        "import scipy.io as sio\n",
        "from tensorflow import keras\n",
        "tf.keras.backend.clear_session()\n",
        "import numpy as np\n",
        "import time\n",
        "from tensorflow.python.keras.layers import Input, Dense, Conv1D, Conv1DTranspose, MaxPool1D, UpSampling1D, concatenate,Flatten,Reshape,Cropping1D\n",
        "from tensorflow.python.keras.models import Model\n",
        "import matplotlib.pyplot as plt\n",
        "\n"
      ],
      "metadata": {
        "id": "6KoexSMT0i5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6FWgjFg0WHL"
      },
      "outputs": [],
      "source": [
        "#import time data\n",
        "# time_data = np.loadtxt('/content/gdrive/MyDrive/analytic_training_data_seqpar/7/time_analytic.txt',delimiter=',')   #sequential\n",
        "time_data = np.loadtxt('/content/gdrive/MyDrive/analytic_training_data/44/time_analytic.txt',delimiter=',')   #parallel\n",
        "\n",
        "#import noise-free data\n",
        "# curve_data = np.loadtxt('/content/gdrive/MyDrive/analytic_training_data_seqpar/7/clean_data_analytic.txt',delimiter=',')\n",
        "curve_data = np.loadtxt('/content/gdrive/MyDrive/analytic_training_data/44/clean_data_analytic.txt',delimiter=',')\n",
        "\n",
        "#import noisy data\n",
        "# noisy_data = np.loadtxt('/content/gdrive/MyDrive/analytic_training_data_seqpar/7/noisy_data_analytic.txt',delimiter=',')\n",
        "noisy_data = np.loadtxt('/content/gdrive/MyDrive/analytic_training_data/44/noisy_data_analytic.txt',delimiter=',')\n",
        "\n",
        "# change axes for plotting later\n",
        "curve_data = np.transpose([curve_data])\n",
        "curve_data = np.swapaxes(curve_data, 0, -1)\n",
        "time_data = np.transpose([time_data])\n",
        "time_data = np.swapaxes(time_data, 0, -1)\n",
        "noisy_data = np.transpose([noisy_data])\n",
        "noisy_data = np.swapaxes(noisy_data, 0, -1)\n",
        "\n",
        "#using np.squeeze to convert curve_data from 3 dimensional to 2 dimentional\n",
        "curve_data_squeezed = np.squeeze(curve_data)\n",
        "noisy_data_squeezed = np.squeeze(noisy_data)\n",
        "\n",
        "#choose what amount of the data set to keep for validation\n",
        "percent_validation = 0.1\n",
        "\n",
        "#uses the above to explicitly set how many data sets in the validation set\n",
        "amount_validation = round(curve_data.shape[1]*percent_validation)\n",
        "\n",
        "#set the training input and output\n",
        "#for noise removal, input will be noisy, output will be noise-free\n",
        "training_input = noisy_data_squeezed[0:noisy_data_squeezed.shape[0]-amount_validation]\n",
        "training_output = curve_data_squeezed[0:curve_data_squeezed.shape[0]-amount_validation]\n",
        "\n",
        "#clear the model (from any previous usage)\n",
        "tf.keras.backend.clear_session()  #found online https://www.tensorflow.org/api_docs/python/tf/keras/backend/clear_session\n",
        "\n",
        "#assign the number of epochs\n",
        "n_epochs = 3000\n",
        "\n",
        "#assign dimensions of convolutional filter\n",
        "s = 40\n",
        "w = 40\n",
        "\n",
        "#include biasing?\n",
        "UB = 'False'   \n",
        "\n",
        "#autoencoder convolutional neural network\n",
        "\n",
        "#take an input of (40,1), this is the number of data points in each data set\n",
        "input = Input((training_input.shape[1],1))\n",
        "\n",
        "#convolution\n",
        "conv1 = Conv1D(s,w,padding='same',activation=tf.nn.relu,use_bias=UB)(input)\n",
        "\n",
        "#max-pooling\n",
        "pool1 = MaxPool1D(2,padding='same')(conv1)\n",
        "\n",
        "#convolution\n",
        "conv2 = Conv1D(2*s,w,padding='same',activation=tf.nn.relu,use_bias=UB)(pool1)\n",
        "\n",
        "#max-pooling\n",
        "pool2 = MaxPool1D(2,padding='same')(conv2)\n",
        "\n",
        "#deconvolution\n",
        "deconv1 = Conv1DTranspose(2*s,w,padding='same',activation=tf.nn.relu,use_bias=UB)(pool2)\n",
        "\n",
        "#concatenation\n",
        "concat1 = concatenate([pool2,deconv1],axis=1)\n",
        "\n",
        "#deconvolution\n",
        "deconv2 = Conv1DTranspose(s,w,padding='same',activation=tf.nn.relu,use_bias=UB)(concat1)\n",
        "\n",
        "#concatenation\n",
        "concat2 = concatenate([pool1,deconv2],axis=1)\n",
        "\n",
        "#set the final outcome as the output\n",
        "output = Conv1D(1,w,padding='same',activation=tf.nn.relu,use_bias=UB)(concat2)\n",
        "\n",
        "#create model\n",
        "model=Model(input,output)\n",
        "\n",
        "#show a summary of dimension manipulation\n",
        "model.summary()\n",
        "\n",
        "#configure the model for training use\n",
        "model.compile(optimizer = keras.optimizers.Adam(learning_rate=0.00010), loss=\"mean_squared_error\", metrics=[\"accuracy\"])\n",
        "\n",
        "#set a checkpoint location\n",
        "checkpoint_path = '/content/gdrive/MyDrive/'\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "#early stopping?\n",
        "#tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=30)\n",
        "\n",
        "cp_callback = [tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, save_weights_only=True, verbose=1, save_freq=1000)]\n",
        "\n",
        "#assign validation data\n",
        "validation_clean = curve_data_squeezed[curve_data_squeezed.shape[0]-amount_validation:curve_data_squeezed.shape[0]] \n",
        "validation_noisy = noisy_data_squeezed[curve_data_squeezed.shape[0]-amount_validation:curve_data_squeezed.shape[0]]\n",
        "\n",
        "validation_input = validation_noisy\n",
        "validation_output = validation_clean\n",
        "\n",
        "#train the network\n",
        "history = model.fit(training_input, training_output, validation_data=(validation_input, validation_output), batch_size=32, epochs=n_epochs, callbacks=cp_callback) #change batch size to 32\n",
        "\n",
        "model.save_weights(checkpoint_path.format(epoch=0))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#load checkpoint\n",
        "model.load_weights(checkpoint_path)\n",
        "\n",
        "#convert to 2 dimensional arrays\n",
        "time_data = np.squeeze(time_data)\n",
        "training_input = np.squeeze(training_input)\n",
        "\n",
        "#initialise empty arrays\n",
        "training_predict = []\n",
        "training_predict_matrix = np.array([])\n",
        "\n",
        "#set lengths of arrays\n",
        "length_curve_data = curve_data.shape[1]\n",
        "length_difference = length_curve_data-amount_validation\n",
        "\n",
        "#stop error messages\n",
        "plt.rcParams.update({'figure.max_open_warning': 0})\n",
        "\n",
        "count = 0\n",
        "\n",
        "#entire dataset\n",
        "for n in range(length_curve_data):\n",
        "  training_predict = np.squeeze(np.squeeze(model.predict(curve_data[:,n,:])))\n",
        "\n",
        "  training_predict_matrix = np.append(training_predict_matrix, training_predict/max(training_predict))\n",
        "\n",
        "#small amount of figures\n",
        "for n in range(length_curve_data-(length_curve_data-10)):\n",
        "  training_predict = np.squeeze(np.squeeze(model.predict(curve_data[:,n,:])))\n",
        "  # plot the training input\n",
        "  plt.plot1 = plt.figure()\n",
        "  plt.plot(time_data,training_input[n,:])\n",
        "  plt.title(\"Validation\"+\" at n= \"+str(n))\n",
        "  plt.savefig(\"/content/gdrive/MyDrive/validation\"+str(n))\n",
        "\n",
        "  #plot the training output\n",
        "  plt.plot2 = plt.figure()\n",
        "  plt.plot(time_data,training_predict/max(training_predict))\n",
        "  plt.title(\"Prediction\"+\" at n= \"+str(n))\n",
        "  plt.savefig(\"/content/gdrive/MyDrive/prediction\"+str(n))\n",
        "\n",
        "  plt.plot3 = plt.figure()\n",
        "  plt.plot(time_data,training_output[n,:]/np.amax(training_output[n,:]))\n",
        "  plt.title(\"Truth\"+\" at n= \"+str(n))\n",
        "  plt.savefig(\"/content/gdrive/MyDrive/expected\"+str(n))\n",
        "\n",
        "  plt.plot4 = plt.figure()\n",
        "  plt.plot(time_data,training_predict/max(training_predict)-training_output[n,:]/np.amax(training_output[n,:]))\n",
        "  plt.title(\"Residual\"+\" at n= \" +str(n))\n",
        "  plt.savefig(\"/content/gdrive/MyDrive/residual\"+str(n))\n",
        "\n",
        "  plt.plot5 = plt.figure()\n",
        "  plt.plot(time_data,training_output[n,:]/np.amax(training_output[n,:]), label=\"Truth\")\n",
        "  plt.plot(time_data,training_predict/max(training_predict), label=\"Prediction\")\n",
        "  plt.title(\"Prediction Plotted on top of Truth\")\n",
        "  plt.legend()\n",
        "  plt.savefig(\"/content/gdrive/MyDrive/preditionexpected\"+str(n))\n",
        "\n",
        "#investigate loss\n",
        "plt.plot6 = plt.figure()\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.ylim([1e-5, 1e-3])\n",
        "\n",
        "np.savetxt(\"/content/gdrive/MyDrive/training_input.txt\", training_input)\n",
        "np.savetxt(\"/content/gdrive/MyDrive/training_output.txt\", training_output)\n",
        "np.savetxt(\"/content/gdrive/MyDrive/time_data.txt\", time_data)\n",
        "np.savetxt(\"/content/gdrive/MyDrive/training_predict_matrix.txt\", training_predict_matrix)"
      ],
      "metadata": {
        "id": "h4rvBGS10eF4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}